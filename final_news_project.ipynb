{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05b7dd0b-a6a3-40bc-b36a-0d7d334de17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries \n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect, DetectorFactory\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b68ddb7-cd99-4d30-a8a6-0b4d64b99890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to get different fields by parsing html files using beautifulsoup4 \n",
    "\n",
    "#extracting title by finding 'h1'\n",
    "def get_title(soup):\n",
    "    try:\n",
    "        return soup.find(\"h1\").text.strip()\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "#extracting summary by getting first 3 'p' tags and joining them\n",
    "def get_summary(soup):\n",
    "    try:\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        return \" \".join(p.text.strip() for p in paragraphs[:3]) if paragraphs else \"Unknown\"\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "#extracting published date by meta tag or time tag\n",
    "def get_published(soup):\n",
    "    try:\n",
    "        meta_time = (\n",
    "            soup.find(\"meta\", {\"property\": \"article:published_time\"}) or\n",
    "            soup.find(\"meta\", {\"name\": \"pubdate\"})\n",
    "        )\n",
    "        if meta_time and meta_time.get(\"content\"):\n",
    "            return meta_time[\"content\"]\n",
    "        time_tag = soup.find(\"time\")\n",
    "        if time_tag and time_tag.get(\"datetime\"):\n",
    "            return time_tag[\"datetime\"]\n",
    "    except:\n",
    "        pass\n",
    "    return \"Unknown\"\n",
    "\n",
    "#extracting the source of the article by meta tag\n",
    "def get_source(soup):\n",
    "    try:\n",
    "        meta = soup.find(\"meta\", property=\"og:site_name\")\n",
    "        return meta[\"content\"].strip() if meta and meta.get(\"content\") else \"Unknown\"\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "#extracting author of the article by meta tag or 'author-name' class in span tag\n",
    "def get_author(soup):\n",
    "    try:\n",
    "        meta_author = soup.find(\"meta\", attrs={\"name\": \"author\"})\n",
    "        if meta_author and meta_author.get(\"content\"):\n",
    "            return meta_author[\"content\"].strip()\n",
    "        author_tag = soup.find(\"span\", class_=\"author-name\")\n",
    "        if author_tag:\n",
    "            return author_tag.text.strip()\n",
    "    except:\n",
    "        pass\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "#also assigning default values as \"unknown\" if not able to find appropriate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5112023-8251-446c-b6ae-1b68f0d74879",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#function for detecting language of the article using 'title'\n",
    "DetectorFactory.seed = 0  \n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        if text and text.strip():  \n",
    "            return detect(text)\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "    except:\n",
    "        return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c30fab5-c3aa-47f3-8648-7464fa337452",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#function for extracting title, summary, published_date, source, author, link of the article \n",
    "\n",
    "\n",
    "def rss_entry(entry, feed_title, country, headers):\n",
    "    title = getattr(entry, 'title', None)\n",
    "    summary = getattr(entry, 'summary', None)\n",
    "    published = getattr(entry, 'published', None)\n",
    "    source = feed_title\n",
    "    author = getattr(entry, 'author', None) or getattr(entry, 'dc_creator', None)\n",
    "    link = getattr(entry, 'link', 'Unknown')\n",
    "\n",
    "    #if not found in the rss feeds, we use request and bs4 for parsing the link of that particular article and try to extract the missing fields\n",
    "    #if still not found , assigning default value as \"unknown\"\n",
    "    soup = None\n",
    "    if not (title and summary and published and author and source) and link != \"Unknown\":\n",
    "        try:\n",
    "            response = requests.get(link, headers=headers, timeout=10)\n",
    "            response.encoding = 'utf-8'\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        except:\n",
    "            soup = None\n",
    "\n",
    "    if not title and soup:\n",
    "        title = get_title(soup)\n",
    "    if not summary and soup:\n",
    "        summary = get_summary(soup)\n",
    "    if not published and soup:\n",
    "        published = get_published(soup)\n",
    "    if not source and soup:\n",
    "        source = get_source(soup)\n",
    "    if not author and soup:\n",
    "        author = get_author(soup)\n",
    "\n",
    "    #returning the news_item , this also includes the language along with the other parsed fields\n",
    "    return {\n",
    "        'News Title': title or \"Unknown\",\n",
    "        'Publication Date': published or \"Unknown\",\n",
    "        'Source (News Agency)': source or \"Unknown\",\n",
    "        'Country': country,\n",
    "        'Summary': summary or \"Unknown\",\n",
    "        'Link to full Article': link,\n",
    "        'Author': author or \"Unknown\",\n",
    "        'Language':detect_language(title)\n",
    "        \n",
    "    }\n",
    "\n",
    "#function for parsing rss feeds by looping through the dictionary and using 'feedparser' for parsing\n",
    "#we call the rss_entry function for extracting the fields and creating a news_item to add in the list\n",
    "\n",
    "def parse_rss_feeds(rss_feeds, headers=None):\n",
    "    if headers is None:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    all_news = []\n",
    "    for country, urls in rss_feeds.items():\n",
    "        for url in urls:\n",
    "            try:\n",
    "                f = feedparser.parse(url)\n",
    "                for entry in f.entries:\n",
    "                    try:\n",
    "                        news_item = rss_entry(entry, getattr(f.feed, 'title', 'Unknown'), country, headers)\n",
    "                        all_news.append(news_item)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing entry from {url}: {e}\")\n",
    "                        continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing feed {url}: {e}\")\n",
    "                continue\n",
    "    return all_news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d155ba-57e0-4b13-9736-d27b20a6c4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#function for handling the historical data\n",
    "#using bs4 for parsing links of some articles that were published more than a year ago\n",
    "\n",
    "def scrape_historical_articles(historical_urls):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    all_news = []\n",
    "\n",
    "    for country, urls in historical_urls.items():\n",
    "        for url in urls:\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers, timeout=10)\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # Using functions to extract the data from the soup\n",
    "                title = get_title(soup)\n",
    "                summary = get_summary(soup)\n",
    "                published = get_published(soup)\n",
    "                source = get_source(soup)\n",
    "                author = get_author(soup)\n",
    "\n",
    "                #creating the news_item to add in the list , including the language of the article detected along with other parsed fields\n",
    "                news_item = {\n",
    "                    'News Title': title,\n",
    "                    'Publication Date': published,\n",
    "                    'Source (News Agency)': source,\n",
    "                    'Country': country,\n",
    "                    'Summary': summary,\n",
    "                    'Link to full Article': url,\n",
    "                    'Author': author,\n",
    "                    'Language': detect_language(title)\n",
    "                }\n",
    "\n",
    "                all_news.append(news_item)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Historical scrape error for {url}: {e}\")\n",
    "                continue\n",
    "\n",
    "    return all_news\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "055db59a-0589-4f0b-93b0-0c8dde430807",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#this is a dictionary of rss feeds \n",
    "#with country as key and a list of links for different newspapers as its value\n",
    "#creating this as parsing country was not possible through rss feeds(feedparser) or html parsing(bs4)\n",
    "#this contains 20+ countries data with 30+ news sites\n",
    "rss_feeds = {\n",
    "    \"UK\": [\n",
    "        \"http://feeds.bbci.co.uk/news/rss.xml\"\n",
    "    ],\n",
    "    \"USA\": [\n",
    "        \"http://rss.cnn.com/rss/edition.rss\",\n",
    "        \"https://www.vox.com/rss/index.xml\",\n",
    "        \"https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml\"\n",
    "    ],\n",
    "    \"Japan\": [\n",
    "        \"https://www3.nhk.or.jp/rss/news/cat0.xml\"\n",
    "    ],\n",
    "    \"India\": [\n",
    "        \"https://timesofindia.indiatimes.com/rssfeeds/-2128936835.cms\"\n",
    "    ],\n",
    "    \"Germany\": [\n",
    "        \"https://rss.dw.com/rdf/rss-en-all\",\n",
    "        \"https://www.spiegel.de/schlagzeilen/index.rss\"\n",
    "    ],\n",
    "    \"France\": [\n",
    "        \"https://www.lemonde.fr/rss/une.xml\"\n",
    "    ],\n",
    "    \"Russia\": [\n",
    "        \"https://tass.com/rss/v2.xml\",\n",
    "        \"https://vc.ru/rss/all\"\n",
    "    ],\n",
    "    \"Australia\": [\n",
    "        \"https://www.abc.net.au/news/feed/51120/rss.xml\"\n",
    "    ],\n",
    "    \"Canada\": [\n",
    "        \"https://www.cbc.ca/cmlink/rss-world\"\n",
    "    ],\n",
    "    \"China\": [\n",
    "        \"http://www.xinhuanet.com/english/rss/worldrss.xml\"\n",
    "    ],\n",
    "    \"Brazil\": [\n",
    "        \"https://g1.globo.com/rss/g1/\"\n",
    "    ],\n",
    "    \"Middle East\": [\n",
    "        \"https://www.aljazeera.com/xml/rss/all.xml\"\n",
    "    ],\n",
    "    \"Netherlands\": [\n",
    "        \"https://www.bellingcat.com/feed/\"\n",
    "    ],\n",
    "    \"Turkey\": [\n",
    "        \"https://www.ahaber.com.tr/rss/dunya.xml\"\n",
    "    ],\n",
    "    \"Vietnam\": [\n",
    "        \"https://vnexpress.net/rss/the-thao.rss\"\n",
    "    ],\n",
    "    \"South Korea\": [\n",
    "        \"http://www.koreatimes.co.kr/www/rss/biz.xml\"\n",
    "    ],\n",
    "    \"Thailand\": [\n",
    "        \"https://www.bangkokpost.com/rss/data/topstories.xml\"\n",
    "    ],\n",
    "    \"Sri Lanka\": [\n",
    "        \"http://www.gossiplankahotnews.com/feeds/posts/default/-/Hotnews\"\n",
    "    ],\n",
    "    \"Hong Kong\": [\n",
    "        \"https://hongkongclimbing.com/feed/\"\n",
    "    ],\n",
    "    \"South Africa\": [\n",
    "        \"https://www.theguardian.com/media/dailymail/rss\"\n",
    "    ],\n",
    "    \"Pakistan\": [\n",
    "        \"http://www.dawn.com/feeds/home\"\n",
    "    ],\n",
    "    \"Ukraine\": [\n",
    "        \"http://euromaidanpress.com/feed/\"\n",
    "    ],\n",
    "    \"Qatar\": [\n",
    "        \"https://dohanews.co/feed/\"\n",
    "    ],\n",
    "    \"Hungary\": [\n",
    "        \"https://dailynewshungary.com/feed/\"\n",
    "    ],\n",
    "    \"Finland\": [\n",
    "        \"http://dailyfinland.fi/feed/latest-rss.xml\"\n",
    "    ],\n",
    "    \"Denmark\": [\n",
    "        \"http://www.esa.int/rssfeed/Denmark\"\n",
    "    ],\n",
    "    \"Portugal\": [\n",
    "        \"https://balcao.portugal2020.pt/nb.balcao2020.ui/concurso/rss/\"\n",
    "    ],\n",
    "    \"Zimbabwe\": [\n",
    "        \"http://www.thezimbabwean.co/feed/\"\n",
    "    ],\n",
    "    \"Iraq\": [\n",
    "        \"http://www.iraq-businessnews.com/feed/\"\n",
    "    ],\n",
    "    \"Greece\": [\n",
    "        \"https://gr.gizchina.com/feed/\"\n",
    "    ],\n",
    "    \"Italy\": [\n",
    "        \"https://www.repubblica.it/rss/homepage/rss2.0.xml\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "#this is the dictionary of urls of the articles that were published more than a year ago\n",
    "#due to time constraint only 5 articles are gathered here\n",
    "#this is also in the same format as country as key and list of links as its value\n",
    "\n",
    "historical_urls = {\n",
    "    \"UK\": [\n",
    "        \"https://www.bbc.com/news/articles/cp00jze920eo\",\n",
    "        \"https://edition.cnn.com/2020/05/14/uk/united-kingdom-divided-approach-coronavirus-intl-gbr\"\n",
    "    ],\n",
    "    \"USA\": [\n",
    "        \"https://edition.cnn.com/2023/01/31/tennis/atp-alexander-zverev-domestic-abuse-spt-intl/index.html\"\n",
    "    ],\n",
    "    \"India\": [\n",
    "        \"https://timesofindia.indiatimes.com/india/2022-when-the-world-shifted-on-its-axis/articleshow/96416470.cms\"\n",
    "    ],\n",
    "    \"Middle East\": [\n",
    "        \"https://network.aljazeera.net/en/pressroom/former-cia-chief-%E2%80%9Cyes%E2%80%9D-donald-trump-%E2%80%9Crecruiting-sergeant%E2%80%9D-isil-0\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdb8aa22-51ce-4860-9d74-6288015dc423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing feed https://www.cbc.ca/cmlink/rss-world: Remote end closed connection without response\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#calling both the functions and creating a list with both the outputs\n",
    "all_news = parse_rss_feeds(rss_feeds) + scrape_historical_articles(historical_urls)\n",
    "\n",
    "#creating a dataframe using pandas\n",
    "df = pd.DataFrame(all_news)\n",
    "df.fillna('Unknown', inplace=True)\n",
    "\n",
    "#handling repeated data if any based on the title and link of the article\n",
    "df.drop_duplicates(subset=['News Title', 'Link to full Article'], inplace=True)\n",
    "\n",
    "#creating a csv file\n",
    "df.to_csv('all_news.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eada289-d7e4-4484-8a87-d58f3ebb4875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQLite database \n",
    "conn = sqlite3.connect('news_data.db')  \n",
    "cursor = conn.cursor()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02eb342c-6c75-49a5-aed2-88ff8bcd298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table if it doesn't exist\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS news (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        title TEXT,\n",
    "        publication_date TEXT,\n",
    "        source TEXT,\n",
    "        country TEXT,\n",
    "        summary TEXT,\n",
    "        link TEXT ,\n",
    "        author TEXT,\n",
    "        language TEXT\n",
    "    )\n",
    "''')\n",
    "\n",
    "\n",
    "# Inserting data into the table\n",
    "for _, row in df.iterrows():\n",
    "    try:\n",
    "        #extracting data from the data frame and inserting into the table in database of SQLite\n",
    "        cursor.execute('''\n",
    "            INSERT OR IGNORE INTO news \n",
    "            (title, publication_date, source, country, summary, link, author, language)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            row['News Title'],\n",
    "            row['Publication Date'],\n",
    "            row['Source (News Agency)'],\n",
    "            row['Country'],\n",
    "            row['Summary'],\n",
    "            row['Link to full Article'],\n",
    "            row['Author'],\n",
    "            row['Language']\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        print(f\"Insert failed: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76c7124e-8c12-4de0-8daf-82f35c7c97c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee39e8ad-b6eb-43c2-b96d-8efba574f26a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>source</th>\n",
       "      <th>country</th>\n",
       "      <th>summary</th>\n",
       "      <th>link</th>\n",
       "      <th>author</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>UK prosecutors say 21 charges authorised again...</td>\n",
       "      <td>Wed, 28 May 2025 16:30:06 GMT</td>\n",
       "      <td>BBC News</td>\n",
       "      <td>UK</td>\n",
       "      <td>Prosecutors in the UK confirm they have author...</td>\n",
       "      <td>https://www.bbc.com/news/articles/ckg41g1140po</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The terrifying new weapon changing the war in ...</td>\n",
       "      <td>Wed, 28 May 2025 16:00:06 GMT</td>\n",
       "      <td>BBC News</td>\n",
       "      <td>UK</td>\n",
       "      <td>Swarms of fibre optic drones give Russia the e...</td>\n",
       "      <td>https://www.bbc.com/news/articles/ckgn47e5qyno</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Heathrow chief asleep as airport closed, inqui...</td>\n",
       "      <td>Wed, 28 May 2025 16:10:06 GMT</td>\n",
       "      <td>BBC News</td>\n",
       "      <td>UK</td>\n",
       "      <td>Findings from a review commissioned by Heathro...</td>\n",
       "      <td>https://www.bbc.com/news/articles/c62n0y3nepzo</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>French paedophile surgeon who abused hundreds ...</td>\n",
       "      <td>Wed, 28 May 2025 12:45:32 GMT</td>\n",
       "      <td>BBC News</td>\n",
       "      <td>UK</td>\n",
       "      <td>Joel Le Scouarnec, who has admitted his guilt,...</td>\n",
       "      <td>https://www.bbc.com/news/articles/cvgdkyge198o</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>BBC Verify examines footage of chaos at aid si...</td>\n",
       "      <td>Wed, 28 May 2025 16:04:34 GMT</td>\n",
       "      <td>BBC News</td>\n",
       "      <td>UK</td>\n",
       "      <td>The UN Human Rights Office has said it believe...</td>\n",
       "      <td>https://www.bbc.com/news/videos/cvgdkgmn3yxo</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>967</td>\n",
       "      <td>India’s opposition looked down and out – now t...</td>\n",
       "      <td>2024-06-05T16:18:18.822Z</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>UK</td>\n",
       "      <td>The results of India’s general election announ...</td>\n",
       "      <td>https://www.bbc.com/news/articles/cp00jze920eo</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>968</td>\n",
       "      <td>The United Kingdom’s four countries take a div...</td>\n",
       "      <td>2020-05-14T06:02:05Z</td>\n",
       "      <td>CNN</td>\n",
       "      <td>UK</td>\n",
       "      <td>The UK’s coronavirus crisis has reignited one ...</td>\n",
       "      <td>https://edition.cnn.com/2020/05/14/uk/united-k...</td>\n",
       "      <td>Luke McGee</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>969</td>\n",
       "      <td>ATP says no disciplinary action to be taken ag...</td>\n",
       "      <td>2023-01-31T13:23:23Z</td>\n",
       "      <td>CNN</td>\n",
       "      <td>USA</td>\n",
       "      <td>The ATP Tour announced Tuesday that there will...</td>\n",
       "      <td>https://edition.cnn.com/2023/01/31/tennis/atp-...</td>\n",
       "      <td>Jill Martin</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>970</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>India</td>\n",
       "      <td>As conflicts become the new normal, countries ...</td>\n",
       "      <td>https://timesofindia.indiatimes.com/india/2022...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>971</td>\n",
       "      <td>Former CIA chief: “Yes”, Donald Trump is a “re...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Al Jazeera Media Network</td>\n",
       "      <td>Middle East</td>\n",
       "      <td>A truly global network ·         Agrees, “yes”...</td>\n",
       "      <td>https://network.aljazeera.net/en/pressroom/for...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>970 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0      1  UK prosecutors say 21 charges authorised again...   \n",
       "1      2  The terrifying new weapon changing the war in ...   \n",
       "2      3  Heathrow chief asleep as airport closed, inqui...   \n",
       "3      4  French paedophile surgeon who abused hundreds ...   \n",
       "4      5  BBC Verify examines footage of chaos at aid si...   \n",
       "..   ...                                                ...   \n",
       "965  967  India’s opposition looked down and out – now t...   \n",
       "966  968  The United Kingdom’s four countries take a div...   \n",
       "967  969  ATP says no disciplinary action to be taken ag...   \n",
       "968  970                                            Unknown   \n",
       "969  971  Former CIA chief: “Yes”, Donald Trump is a “re...   \n",
       "\n",
       "                  publication_date                    source      country  \\\n",
       "0    Wed, 28 May 2025 16:30:06 GMT                  BBC News           UK   \n",
       "1    Wed, 28 May 2025 16:00:06 GMT                  BBC News           UK   \n",
       "2    Wed, 28 May 2025 16:10:06 GMT                  BBC News           UK   \n",
       "3    Wed, 28 May 2025 12:45:32 GMT                  BBC News           UK   \n",
       "4    Wed, 28 May 2025 16:04:34 GMT                  BBC News           UK   \n",
       "..                             ...                       ...          ...   \n",
       "965       2024-06-05T16:18:18.822Z                   Unknown           UK   \n",
       "966           2020-05-14T06:02:05Z                       CNN           UK   \n",
       "967           2023-01-31T13:23:23Z                       CNN          USA   \n",
       "968                        Unknown        The Times of India        India   \n",
       "969                        Unknown  Al Jazeera Media Network  Middle East   \n",
       "\n",
       "                                               summary  \\\n",
       "0    Prosecutors in the UK confirm they have author...   \n",
       "1    Swarms of fibre optic drones give Russia the e...   \n",
       "2    Findings from a review commissioned by Heathro...   \n",
       "3    Joel Le Scouarnec, who has admitted his guilt,...   \n",
       "4    The UN Human Rights Office has said it believe...   \n",
       "..                                                 ...   \n",
       "965  The results of India’s general election announ...   \n",
       "966  The UK’s coronavirus crisis has reignited one ...   \n",
       "967  The ATP Tour announced Tuesday that there will...   \n",
       "968  As conflicts become the new normal, countries ...   \n",
       "969  A truly global network ·         Agrees, “yes”...   \n",
       "\n",
       "                                                  link       author language  \n",
       "0       https://www.bbc.com/news/articles/ckg41g1140po      Unknown       en  \n",
       "1       https://www.bbc.com/news/articles/ckgn47e5qyno      Unknown       en  \n",
       "2       https://www.bbc.com/news/articles/c62n0y3nepzo      Unknown       en  \n",
       "3       https://www.bbc.com/news/articles/cvgdkyge198o      Unknown       en  \n",
       "4         https://www.bbc.com/news/videos/cvgdkgmn3yxo      Unknown       en  \n",
       "..                                                 ...          ...      ...  \n",
       "965     https://www.bbc.com/news/articles/cp00jze920eo      Unknown       en  \n",
       "966  https://edition.cnn.com/2020/05/14/uk/united-k...   Luke McGee       en  \n",
       "967  https://edition.cnn.com/2023/01/31/tennis/atp-...  Jill Martin       en  \n",
       "968  https://timesofindia.indiatimes.com/india/2022...      Unknown       en  \n",
       "969  https://network.aljazeera.net/en/pressroom/for...      Unknown       en  \n",
       "\n",
       "[970 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#checking after insertion , if the records were added successfully or not\n",
    "conn = sqlite3.connect('news_data.db')\n",
    "df2 = pd.read_sql_query(\"SELECT * FROM news\", conn)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "df2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
